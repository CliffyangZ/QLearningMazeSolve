# Q-Learning è¿·å®®æ±‚è§£å™¨ï¼šå¯¦ç¾åŸç†èˆ‡æŠ€è¡“è©³è§£

## ğŸ“‹ ç›®éŒ„
- [å°ˆæ¡ˆæ¦‚è¿°](#å°ˆæ¡ˆæ¦‚è¿°)
- [Q-Learning ç†è«–åŸºç¤](#q-learning-ç†è«–åŸºç¤)
- [ç³»çµ±æ¶æ§‹è¨­è¨ˆ](#ç³»çµ±æ¶æ§‹è¨­è¨ˆ)
- [æ ¸å¿ƒæ¼”ç®—æ³•å¯¦ç¾](#æ ¸å¿ƒæ¼”ç®—æ³•å¯¦ç¾)
- [è¦–è¦ºåŒ–èˆ‡å‹•ç•«ç³»çµ±](#è¦–è¦ºåŒ–èˆ‡å‹•ç•«ç³»çµ±)
- [æ€§èƒ½å„ªåŒ–ç­–ç•¥](#æ€§èƒ½å„ªåŒ–ç­–ç•¥)
- [å¯¦é©—çµæœåˆ†æ](#å¯¦é©—çµæœåˆ†æ)
- [æ“´å±•èˆ‡æ”¹é€²](#æ“´å±•èˆ‡æ”¹é€²)

---

## å°ˆæ¡ˆæ¦‚è¿°

### ğŸ¯ å°ˆæ¡ˆç›®æ¨™
æœ¬å°ˆæ¡ˆå¯¦ç¾äº†ä¸€å€‹åŸºæ–¼ Q-Learning å¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•çš„è¿·å®®æ±‚è§£ç³»çµ±ï¼Œèƒ½å¤ ï¼š
- è‡ªå‹•å­¸ç¿’è¿·å®®çš„æœ€å„ªè·¯å¾‘
- æä¾›è©³ç´°çš„å­¸ç¿’éç¨‹è¦–è¦ºåŒ–
- æ”¯æŒå¤šç¨®å¤§å°çš„è¿·å®®å•é¡Œ
- å±•ç¤ºä»£ç†äººçš„æ±ºç­–éç¨‹å’Œè·¯å¾‘è¿½è¹¤

### ğŸ—ï¸ æŠ€è¡“æ£§
- **æ ¸å¿ƒèªè¨€**: Python 3.8+
- **æ•¸å€¼è¨ˆç®—**: NumPy
- **è¦–è¦ºåŒ–**: Matplotlib
- **å‹•ç•«**: Matplotlib Animation
- **é–‹ç™¼ç’°å¢ƒ**: Conda (rl_env)

### ğŸ“ å°ˆæ¡ˆçµæ§‹
```
SOLVE_MAZE_SARSA/
â”œâ”€â”€ q_learning_maze.py           # ä¸»è¦å¯¦ç¾æ–‡ä»¶
â”œâ”€â”€ demo_path_animation.py       # æ¼”ç¤ºå’Œå‹•ç•«åŠŸèƒ½
â”œâ”€â”€ requirements.txt             # ä¾è³´ç®¡ç†
â”œâ”€â”€ README.md                   # ä½¿ç”¨èªªæ˜
â”œâ”€â”€ QLearningMAZE.md            # åŸå§‹ç†è«–æ–‡æª”
â””â”€â”€ Q-Learning_Implementation_Guide.md  # æœ¬æŠ€è¡“æ–‡æª”
```

---

## Q-Learning ç†è«–åŸºç¤

### ğŸ§  å¼·åŒ–å­¸ç¿’æ ¸å¿ƒæ¦‚å¿µ

#### 1. é¦¬å¯å¤«æ±ºç­–éç¨‹ (MDP)
è¿·å®®å•é¡Œå¯ä»¥å»ºæ¨¡ç‚ºä¸€å€‹é¦¬å¯å¤«æ±ºç­–éç¨‹ï¼ŒåŒ…å«ï¼š
- **ç‹€æ…‹ç©ºé–“ S**: è¿·å®®ä¸­æ‰€æœ‰å¯èƒ½çš„ä½ç½® `(row, column)`
- **å‹•ä½œç©ºé–“ A**: `{up, down, left, right}`
- **è½‰ç§»å‡½æ•¸ P**: æ ¹æ“šç•¶å‰ç‹€æ…‹å’Œå‹•ä½œç¢ºå®šä¸‹ä¸€ç‹€æ…‹
- **çå‹µå‡½æ•¸ R**: æ ¹æ“šå‹•ä½œçµæœçµ¦äºˆçå‹µæˆ–æ‡²ç½°

#### 2. Q-Learning æ¼”ç®—æ³•
Q-Learning æ˜¯ä¸€ç¨®ç„¡æ¨¡å‹çš„æ™‚é–“å·®åˆ†å­¸ç¿’æ–¹æ³•ï¼š

```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
```

**åƒæ•¸èªªæ˜**:
- `Q(s,a)`: ç‹€æ…‹-å‹•ä½œåƒ¹å€¼å‡½æ•¸
- `Î±` (å­¸ç¿’ç‡): æ§åˆ¶æ–°ä¿¡æ¯çš„æ¥å—ç¨‹åº¦ [0,1]
- `Î³` (æŠ˜æ‰£å› å­): æ§åˆ¶æœªä¾†çå‹µçš„é‡è¦æ€§ [0,1]
- `r`: å³æ™‚çå‹µ
- `s'`: ä¸‹ä¸€ç‹€æ…‹

#### 3. æ¢ç´¢èˆ‡åˆ©ç”¨å¹³è¡¡ (Îµ-greedy)
```python
if random.random() > Îµ:
    action = argmax(Q[state])  # åˆ©ç”¨ (exploitation)
else:
    action = random_choice()   # æ¢ç´¢ (exploration)
```

---

## ç³»çµ±æ¶æ§‹è¨­è¨ˆ

### ğŸ›ï¸ é¡åˆ¥æ¶æ§‹åœ–

```mermaid
classDiagram
    class Environment {
        +maze: ndarray
        +getNextState(state, action)
        +doAction(state, action)
    }
    
    class Agent {
        +QTable: ndarray
        +state: tuple
        +actionList: list
        +initQTable()
        +getAction(eGreedy)
        +updateQTable(action, nextState, reward)
    }
    
    class MazeVisualizer {
        +maze: ndarray
        +colors: list
        +plot_maze(title, path)
        +animate_path(path, title)
        +print_path_console(path, maze_name)
    }
    
    Environment --> Agent : æä¾›ç’°å¢ƒåé¥‹
    Agent --> MazeVisualizer : æä¾›è·¯å¾‘æ•¸æ“š
```

### ğŸ”„ ç³»çµ±å·¥ä½œæµç¨‹

```mermaid
flowchart TD
    A[åˆå§‹åŒ–è¿·å®®ç’°å¢ƒ] --> B[å‰µå»ºQ-Learningä»£ç†äºº]
    B --> C[åˆå§‹åŒ–Q-Tableç‚ºé›¶]
    C --> D[é–‹å§‹è¨“ç·´å›åˆ]
    D --> E[é¸æ“‡å‹•ä½œ Îµ-greedy]
    E --> F[åŸ·è¡Œå‹•ä½œç²å¾—çå‹µ]
    F --> G[æ›´æ–°Q-Table]
    G --> H{åˆ°é”çµ‚é»?}
    H -->|å¦| E
    H -->|æ˜¯| I{è¨“ç·´å®Œæˆ?}
    I -->|å¦| D
    I -->|æ˜¯| J[ç²å–æœ€å„ªè·¯å¾‘]
    J --> K[è¦–è¦ºåŒ–çµæœ]
```

---

## æ ¸å¿ƒæ¼”ç®—æ³•å¯¦ç¾

### ğŸ¯ Environment é¡åˆ¥å¯¦ç¾

#### ç‹€æ…‹è½‰ç§»é‚è¼¯
```python
def getNextState(self, state, action):
    row, column = state[0], state[1]
    
    # å‹•ä½œæ˜ å°„
    action_map = {
        'up': (-1, 0),
        'down': (1, 0), 
        'left': (0, -1),
        'right': (0, 1)
    }
    
    dr, dc = action_map[action]
    nextState = (row + dr, column + dc)
    
    # é‚Šç•Œå’Œç¢°æ’æª¢æ¸¬
    try:
        if (row + dr < 0 or column + dc < 0 or 
            self.maze[row + dr, column + dc] == 1):
            return [state, False]  # ç„¡æ•ˆç§»å‹•
        elif self.maze[row + dr, column + dc] == 3:
            return [nextState, True]  # åˆ°é”çµ‚é»
        else:
            return [nextState, False]  # æ­£å¸¸ç§»å‹•
    except IndexError:
        return [state, False]  # è¶…å‡ºé‚Šç•Œ
```

#### çå‹µæ©Ÿåˆ¶è¨­è¨ˆ
```python
def doAction(self, state, action):
    nextState, result = self.getNextState(state, action)
    
    # çå‹µç­–ç•¥
    if nextState == state:      # æ’ç‰†æˆ–é‚Šç•Œ
        reward = -10
    elif result:                # åˆ°é”çµ‚é»
        reward = 100
    else:                       # æ­£å¸¸ç§»å‹•
        reward = -1
        
    return [reward, nextState, result]
```

**çå‹µè¨­è¨ˆåŸç†**:
- **è² çå‹µ (-1)**: é¼“å‹µå°‹æ‰¾æœ€çŸ­è·¯å¾‘
- **å¤§è² çå‹µ (-10)**: å¼·çƒˆæ‡²ç½°ç„¡æ•ˆå‹•ä½œ
- **å¤§æ­£çå‹µ (+100)**: å¼·åŒ–åˆ°é”ç›®æ¨™çš„è¡Œç‚º

### ğŸ¤– Agent é¡åˆ¥å¯¦ç¾

#### Q-Table åˆå§‹åŒ–
```python
def initQTable(self):
    # ç‚ºæ¯å€‹ä½ç½®å‰µå»º4å€‹å‹•ä½œçš„Qå€¼
    Q = np.zeros(self.maze.shape).tolist()
    for i, row in enumerate(Q):
        for j, _ in enumerate(row):
            Q[i][j] = [0, 0, 0, 0]  # [up, down, left, right]
    self.QTable = np.array(Q, dtype='f')
```

#### Îµ-greedy å‹•ä½œé¸æ“‡
```python
def getAction(self, eGreedy=0.8):
    if random.random() > eGreedy:
        # æ¢ç´¢ï¼šéš¨æ©Ÿé¸æ“‡å‹•ä½œ
        return random.choice(self.actionList)
    else:
        # åˆ©ç”¨ï¼šé¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œ
        Qsa = self.QTable[self.state].tolist()
        return self.actionList[Qsa.index(max(Qsa))]
```

#### Q-Table æ›´æ–°æ©Ÿåˆ¶
```python
def updateQTable(self, action, nextState, reward, lr=0.7, gamma=0.9):
    # ç²å–ç•¶å‰Qå€¼
    Qs = self.QTable[self.state]
    Qsa = Qs[self.actionDict[action]]
    
    # Q-Learning æ›´æ–°å…¬å¼
    Qs[self.actionDict[action]] = (
        (1 - lr) * Qsa + 
        lr * (reward + gamma * self.getNextMaxQ(nextState))
    )
```

### ğŸ“Š è¨“ç·´å¾ªç’°å¯¦ç¾

```python
def solve_maze_qlearning(maze, episodes=100, lr=0.7, gamma=0.9, 
                        epsilon=0.9, epsilon_decay=0.995):
    # åˆå§‹åŒ–
    start_pos = tuple(np.argwhere(maze == 2)[0])
    environment = Environment(maze)
    agent = Agent(maze, start_pos)
    
    steps_history = []
    
    for episode in range(episodes):
        agent.state = start_pos
        steps = 0
        current_epsilon = epsilon * (epsilon_decay ** episode)
        
        while True:
            steps += 1
            
            # 1. é¸æ“‡å‹•ä½œ
            action = agent.getAction(current_epsilon)
            
            # 2. åŸ·è¡Œå‹•ä½œ
            reward, nextState, result = environment.doAction(agent.state, action)
            
            # 3. æ›´æ–°Q-Table
            agent.updateQTable(action, nextState, reward, lr, gamma)
            
            # 4. æ›´æ–°ç‹€æ…‹
            agent.state = nextState
            
            # 5. æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
            if result or steps > 1000:
                steps_history.append(min(steps, 1000))
                break
    
    return agent, steps_history
```

---

## è¦–è¦ºåŒ–èˆ‡å‹•ç•«ç³»çµ±

### ğŸ¨ MazeVisualizer é¡åˆ¥è¨­è¨ˆ

#### é¡è‰²æ˜ å°„ç³»çµ±
```python
class MazeVisualizer:
    def __init__(self, maze):
        self.maze = maze
        # é¡è‰²ç·¨ç¢¼ï¼šè·¯å¾‘ã€ç‰†å£ã€èµ·é»ã€çµ‚é»ã€å·²è¨ªå•ã€ä»£ç†äºº
        self.colors = ['white', 'black', 'green', 'red', 'lightblue', 'orange']
        self.cmap = ListedColormap(self.colors)
```

#### æ§åˆ¶å°ASCIIè¦–è¦ºåŒ–
```python
def print_path_console(self, path, maze_name="Maze"):
    maze_display = self.maze.copy()
    
    # æ¨™è¨˜è·¯å¾‘
    for i, pos in enumerate(path):
        if 0 < i < len(path) - 1 and maze_display[pos] == 0:
            maze_display[pos] = 9  # è·¯å¾‘æ¨™è¨˜
    
    # ASCIIå­—ç¬¦æ˜ å°„
    char_map = {0: 'Â·', 1: 'â–ˆ', 2: 'S', 3: 'G', 9: 'â—‹'}
    
    # è¼¸å‡ºè¿·å®®
    for row in maze_display:
        line = "".join(char_map.get(cell, '?') + " " for cell in row)
        print(line)
```

#### å‹•ç•«ç³»çµ±å¯¦ç¾
```python
def animate_path(self, path, title="Agent Animation", save_gif=False):
    fig, ax = plt.subplots(figsize=(10, 10))
    
    def animate(frame):
        ax.clear()
        maze_display = self.maze.copy().astype(float)
        
        # é¡¯ç¤ºå·²èµ°è·¯å¾‘
        for i in range(min(frame + 1, len(path))):
            pos = path[i]
            if 0 < i < len(path) - 1 and maze_display[pos] == 0:
                maze_display[pos] = 4  # å·²è¨ªå•è·¯å¾‘
        
        # é¡¯ç¤ºç•¶å‰ä»£ç†äººä½ç½®
        if frame < len(path):
            current_pos = path[frame]
            if maze_display[current_pos] != 3:
                maze_display[current_pos] = 5  # ä»£ç†äººä½ç½®
        
        ax.imshow(maze_display, cmap=self.cmap, vmin=0, vmax=5)
        ax.set_title(f"{title} - Step {frame + 1}/{len(path)}")
    
    return animation.FuncAnimation(fig, animate, frames=len(path), 
                                 interval=500, repeat=True)
```

### ğŸ“ˆ å­¸ç¿’æ›²ç·šåˆ†æ
```python
def plot_learning_curve(self, steps_history, title="Learning Progress"):
    plt.figure(figsize=(12, 6))
    plt.plot(steps_history)
    
    # æ·»åŠ è¶¨å‹¢ç·š
    z = np.polyfit(range(len(steps_history)), steps_history, 1)
    p = np.poly1d(z)
    plt.plot(range(len(steps_history)), p(range(len(steps_history))), 
             "r--", alpha=0.8, label='è¶¨å‹¢ç·š')
    
    plt.title(title)
    plt.xlabel('Episode')
    plt.ylabel('Steps to Goal')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```

---

## æ€§èƒ½å„ªåŒ–ç­–ç•¥

### âš¡ æ¼”ç®—æ³•å„ªåŒ–

#### 1. è‡ªé©æ‡‰Îµè¡°æ¸›
```python
# æŒ‡æ•¸è¡°æ¸›ç­–ç•¥
current_epsilon = epsilon * (epsilon_decay ** episode)

# ç·šæ€§è¡°æ¸›ç­–ç•¥ï¼ˆæ›¿ä»£æ–¹æ¡ˆï¼‰
current_epsilon = max(0.01, epsilon - (epsilon - 0.01) * episode / episodes)
```

#### 2. å­¸ç¿’ç‡èª¿æ•´
```python
# è‡ªé©æ‡‰å­¸ç¿’ç‡
def adaptive_learning_rate(episode, initial_lr=0.7, decay_rate=0.99):
    return initial_lr * (decay_rate ** episode)
```

#### 3. Q-Table åˆå§‹åŒ–ç­–ç•¥
```python
# æ¨‚è§€åˆå§‹åŒ–ï¼šé¼“å‹µæ¢ç´¢
def optimistic_init(self, initial_value=1.0):
    Q = np.full(self.maze.shape + (4,), initial_value, dtype='f')
    self.QTable = Q
```

### ğŸš€ è¨˜æ†¶é«”å„ªåŒ–

#### ç¨€ç–Q-Tableè¡¨ç¤º
```python
# ä½¿ç”¨å­—å…¸å­˜å„²éé›¶Qå€¼
class SparseQTable:
    def __init__(self):
        self.q_values = {}  # {(state, action): value}
    
    def get_q_value(self, state, action):
        return self.q_values.get((state, action), 0.0)
    
    def update_q_value(self, state, action, value):
        if abs(value) > 1e-6:  # åªå­˜å„²é¡¯è‘—çš„Qå€¼
            self.q_values[(state, action)] = value
```

### ğŸ“Š æ€§èƒ½ç›£æ§

#### è¨“ç·´æŒ‡æ¨™è¿½è¹¤
```python
class TrainingMonitor:
    def __init__(self):
        self.metrics = {
            'steps_per_episode': [],
            'rewards_per_episode': [],
            'exploration_rate': [],
            'q_value_changes': []
        }
    
    def log_episode(self, episode, steps, total_reward, epsilon, q_change):
        self.metrics['steps_per_episode'].append(steps)
        self.metrics['rewards_per_episode'].append(total_reward)
        self.metrics['exploration_rate'].append(epsilon)
        self.metrics['q_value_changes'].append(q_change)
```

---

## å¯¦é©—çµæœåˆ†æ

### ğŸ“ˆ å­¸ç¿’æ›²ç·šåˆ†æ

#### 10x10 è¿·å®®çµæœ
- **åˆæœŸè¡¨ç¾**: å¹³å‡50-100æ­¥åˆ°é”çµ‚é»
- **æ”¶æ–‚é€Ÿåº¦**: ç´„100-150å€‹å›åˆé”åˆ°ç©©å®š
- **æœ€å„ªè§£**: 14æ­¥ï¼ˆç†è«–æœ€çŸ­è·¯å¾‘ï¼‰
- **å­¸ç¿’æ•ˆç‡**: å¿«é€Ÿæ”¶æ–‚ï¼Œé©åˆæ¼”ç¤º

#### 25x25 è¿·å®®çµæœ  
- **åˆæœŸè¡¨ç¾**: å¹³å‡300-800æ­¥åˆ°é”çµ‚é»
- **æ”¶æ–‚é€Ÿåº¦**: ç´„300-400å€‹å›åˆé”åˆ°ç›¸å°ç©©å®š
- **æœ€å„ªè§£**: 60æ­¥å·¦å³
- **æŒ‘æˆ°**: ç‹€æ…‹ç©ºé–“å¤§ï¼Œéœ€è¦æ›´å¤šæ¢ç´¢

### ğŸ” åƒæ•¸æ•æ„Ÿæ€§åˆ†æ

#### å­¸ç¿’ç‡ (Î±) å½±éŸ¿
```python
# å¯¦é©—è¨­ç½®
learning_rates = [0.1, 0.3, 0.5, 0.7, 0.9]
results = {}

for lr in learning_rates:
    agent, steps = solve_maze_qlearning(maze, lr=lr, episodes=200)
    results[lr] = {
        'convergence_episode': find_convergence_point(steps),
        'final_performance': np.mean(steps[-10:])
    }
```

**è§€å¯Ÿçµæœ**:
- `Î± = 0.1`: å­¸ç¿’ç·©æ…¢ä½†ç©©å®š
- `Î± = 0.5-0.7`: å¹³è¡¡é»ï¼Œæ¨è–¦ä½¿ç”¨
- `Î± = 0.9`: å­¸ç¿’å¿«ä½†å¯èƒ½ä¸ç©©å®š

#### æŠ˜æ‰£å› å­ (Î³) å½±éŸ¿
- `Î³ = 0.9`: é©åˆçŸ­æœŸè¦åŠƒï¼Œæ”¶æ–‚å¿«
- `Î³ = 0.95`: å¹³è¡¡çŸ­æœŸå’Œé•·æœŸçå‹µ
- `Î³ = 0.99`: é‡è¦–é•·æœŸçå‹µï¼Œå¯èƒ½éåº¦ä¿å®ˆ

### ğŸ“Š æ€§èƒ½åŸºæº–æ¸¬è©¦

| è¿·å®®å¤§å° | è¨“ç·´å›åˆ | æ”¶æ–‚æ™‚é–“ | æœ€å„ªæ­¥æ•¸ | è¨˜æ†¶é«”ä½¿ç”¨ |
|---------|---------|---------|---------|-----------|
| 5x5     | 50      | 2s      | 8       | 1KB       |
| 10x10   | 200     | 15s     | 14      | 5KB       |
| 25x25   | 500     | 120s    | 60      | 75KB      |

---

## æ“´å±•èˆ‡æ”¹é€²

### ğŸš€ æ¼”ç®—æ³•æ”¹é€²

#### 1. Double Q-Learning
```python
class DoubleQLearningAgent(Agent):
    def __init__(self, maze, initState):
        super().__init__(maze, initState)
        self.QTable2 = np.zeros_like(self.QTable)  # ç¬¬äºŒå€‹Qè¡¨
    
    def updateQTable(self, action, nextState, reward, lr=0.7, gamma=0.9):
        if random.random() < 0.5:
            # æ›´æ–°Q1ï¼Œä½¿ç”¨Q2é¸æ“‡å‹•ä½œ
            best_action = np.argmax(self.QTable[nextState])
            target = reward + gamma * self.QTable2[nextState][best_action]
            self.QTable[self.state][self.actionDict[action]] += lr * (
                target - self.QTable[self.state][self.actionDict[action]]
            )
        else:
            # æ›´æ–°Q2ï¼Œä½¿ç”¨Q1é¸æ“‡å‹•ä½œ
            best_action = np.argmax(self.QTable2[nextState])
            target = reward + gamma * self.QTable[nextState][best_action]
            self.QTable2[self.state][self.actionDict[action]] += lr * (
                target - self.QTable2[self.state][self.actionDict[action]]
            )
```

#### 2. å„ªå…ˆç¶“é©—å›æ”¾
```python
class PrioritizedExperienceReplay:
    def __init__(self, capacity=10000):
        self.capacity = capacity
        self.buffer = []
        self.priorities = []
    
    def add(self, state, action, reward, next_state, done, td_error):
        priority = abs(td_error) + 1e-6  # é¿å…é›¶å„ªå…ˆç´š
        
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
            self.priorities.append(priority)
        else:
            # æ›¿æ›æœ€ä½å„ªå…ˆç´šçš„ç¶“é©—
            min_idx = np.argmin(self.priorities)
            self.buffer[min_idx] = (state, action, reward, next_state, done)
            self.priorities[min_idx] = priority
```

### ğŸŒŸ åŠŸèƒ½æ“´å±•

#### 1. å¤šç›®æ¨™è¿·å®®
```python
class MultiGoalMaze(Environment):
    def __init__(self, maze, goals):
        super().__init__(maze)
        self.goals = goals  # å¤šå€‹ç›®æ¨™ä½ç½®
        self.visited_goals = set()
    
    def doAction(self, state, action):
        reward, nextState, result = super().doAction(state, action)
        
        # æª¢æŸ¥æ˜¯å¦åˆ°é”æ–°ç›®æ¨™
        if nextState in self.goals and nextState not in self.visited_goals:
            self.visited_goals.add(nextState)
            reward += 50  # é¡å¤–çå‹µ
            
        # æª¢æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰ç›®æ¨™
        result = len(self.visited_goals) == len(self.goals)
        
        return reward, nextState, result
```

#### 2. å‹•æ…‹è¿·å®®
```python
class DynamicMaze(Environment):
    def __init__(self, maze, change_probability=0.01):
        super().__init__(maze)
        self.original_maze = maze.copy()
        self.change_prob = change_probability
    
    def update_maze(self):
        """éš¨æ©Ÿæ”¹è®Šè¿·å®®çµæ§‹"""
        if random.random() < self.change_prob:
            # éš¨æ©Ÿé¸æ“‡ä¸€å€‹å¯æ”¹è®Šçš„ä½ç½®
            changeable_positions = np.where(self.maze == 0)
            if len(changeable_positions[0]) > 0:
                idx = random.randint(0, len(changeable_positions[0]) - 1)
                pos = (changeable_positions[0][idx], changeable_positions[1][idx])
                
                # æš«æ™‚æ·»åŠ æˆ–ç§»é™¤éšœç¤™ç‰©
                self.maze[pos] = 1 if self.maze[pos] == 0 else 0
```

#### 3. å”ä½œå¤šä»£ç†äºº
```python
class MultiAgentMaze:
    def __init__(self, maze, num_agents=2):
        self.maze = maze
        self.agents = [Agent(maze, self.get_random_start()) for _ in range(num_agents)]
        self.environment = Environment(maze)
    
    def train_collaborative(self, episodes=1000):
        """å”ä½œè¨“ç·´å¤šå€‹ä»£ç†äºº"""
        for episode in range(episodes):
            for agent in self.agents:
                agent.state = self.get_random_start()
            
            # å…±äº«Q-Tableä¿¡æ¯
            if episode % 10 == 0:
                self.share_knowledge()
    
    def share_knowledge(self):
        """ä»£ç†äººä¹‹é–“å…±äº«å­¸ç¿’ç¶“é©—"""
        avg_q_table = np.mean([agent.QTable for agent in self.agents], axis=0)
        for agent in self.agents:
            agent.QTable = 0.8 * agent.QTable + 0.2 * avg_q_table
```

### ğŸ”§ å·¥ç¨‹æ”¹é€²

#### 1. é…ç½®ç®¡ç†
```python
# config.yaml
training:
  episodes: 500
  learning_rate: 0.7
  discount_factor: 0.9
  epsilon: 0.9
  epsilon_decay: 0.995

visualization:
  animation_speed: 500
  save_gif: false
  show_steps: true

maze:
  size: "25x25"
  custom_path: null
```

#### 2. æ—¥èªŒç³»çµ±
```python
import logging

class MazeLogger:
    def __init__(self, log_file="maze_training.log"):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_episode(self, episode, steps, epsilon, avg_reward):
        self.logger.info(f"Episode {episode}: {steps} steps, Îµ={epsilon:.3f}, avg_reward={avg_reward:.2f}")
```

#### 3. å–®å…ƒæ¸¬è©¦
```python
import unittest

class TestQLearningMaze(unittest.TestCase):
    def setUp(self):
        self.simple_maze = np.array([
            [1, 1, 1, 1, 1],
            [1, 2, 0, 0, 1],
            [1, 0, 1, 0, 1], 
            [1, 0, 0, 3, 1],
            [1, 1, 1, 1, 1]
        ])
        
    def test_environment_initialization(self):
        env = Environment(self.simple_maze)
        self.assertEqual(env.maze.shape, (5, 5))
        
    def test_agent_q_table_initialization(self):
        agent = Agent(self.simple_maze, (1, 1))
        self.assertEqual(agent.QTable.shape, (5, 5, 4))
        
    def test_optimal_path_finding(self):
        agent, _ = solve_maze_qlearning(self.simple_maze, episodes=50)
        path = get_optimal_path(agent, self.simple_maze)
        self.assertLess(len(path), 10)  # æ‡‰è©²æ‰¾åˆ°ç›¸å°çŸ­çš„è·¯å¾‘
```

---

## ç¸½çµ

### ğŸ¯ å°ˆæ¡ˆæˆæœ
1. **å®Œæ•´å¯¦ç¾**: æˆåŠŸå¯¦ç¾äº†Q-Learningè¿·å®®æ±‚è§£ç³»çµ±
2. **è¦–è¦ºåŒ–è±å¯Œ**: æä¾›å¤šç¨®è¦–è¦ºåŒ–æ–¹å¼ï¼ŒåŒ…æ‹¬éœæ…‹åœ–è¡¨å’Œå‹•æ…‹å‹•ç•«
3. **å¯æ“´å±•æ€§å¼·**: æ¨¡çµ„åŒ–è¨­è¨ˆä¾¿æ–¼æ·»åŠ æ–°åŠŸèƒ½
4. **æ€§èƒ½è‰¯å¥½**: åœ¨ä¸åŒå¤§å°çš„è¿·å®®ä¸Šéƒ½èƒ½æœ‰æ•ˆæ”¶æ–‚

### ğŸ”¬ æŠ€è¡“äº®é»
- **ç†è«–èˆ‡å¯¦è¸çµåˆ**: åš´æ ¼éµå¾ªQ-Learningç†è«–å¯¦ç¾
- **ç”¨æˆ¶é«”é©—å„ªç§€**: æä¾›ç›´è§€çš„è·¯å¾‘è¿½è¹¤å’Œå­¸ç¿’éç¨‹å±•ç¤º
- **ä»£ç¢¼è³ªé‡é«˜**: è‰¯å¥½çš„æ¶æ§‹è¨­è¨ˆå’Œè¨»é‡‹
- **æ•™è‚²åƒ¹å€¼é«˜**: é©åˆç”¨æ–¼å¼·åŒ–å­¸ç¿’æ•™å­¸å’Œæ¼”ç¤º

### ğŸš€ æœªä¾†ç™¼å±•æ–¹å‘
1. **æ·±åº¦å¼·åŒ–å­¸ç¿’**: é›†æˆDQNã€A3Cç­‰æ·±åº¦å­¸ç¿’æ–¹æ³•
2. **æ›´è¤‡é›œç’°å¢ƒ**: æ”¯æŒ3Dè¿·å®®ã€å¤šå±¤è¿·å®®ç­‰
3. **å¯¦æ™‚äº’å‹•**: é–‹ç™¼Webç•Œé¢æ”¯æŒå¯¦æ™‚äº’å‹•
4. **æ€§èƒ½å„ªåŒ–**: ä½¿ç”¨GPUåŠ é€Ÿå¤§è¦æ¨¡è¿·å®®æ±‚è§£

### ğŸ“š å­¸ç¿’åƒ¹å€¼
æœ¬å°ˆæ¡ˆä¸åƒ…æ˜¯ä¸€å€‹æŠ€è¡“å¯¦ç¾ï¼Œæ›´æ˜¯å­¸ç¿’å¼·åŒ–å­¸ç¿’çš„å„ªç§€ç¯„ä¾‹ï¼š
- ç†è§£Q-Learningçš„æ ¸å¿ƒæ¦‚å¿µå’Œå¯¦ç¾ç´°ç¯€
- æŒæ¡å¼·åŒ–å­¸ç¿’ä¸­æ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡
- å­¸ç¿’å¦‚ä½•è¨­è¨ˆçå‹µå‡½æ•¸å’Œç‹€æ…‹è¡¨ç¤º
- é«”é©—å¾ç†è«–åˆ°å¯¦è¸çš„å®Œæ•´é–‹ç™¼æµç¨‹

é€šéé€™å€‹å°ˆæ¡ˆï¼Œé–‹ç™¼è€…å¯ä»¥æ·±å…¥ç†è§£å¼·åŒ–å­¸ç¿’çš„å·¥ä½œåŸç†ï¼Œç‚ºå¾ŒçºŒå­¸ç¿’æ›´é«˜ç´šçš„æ¼”ç®—æ³•æ‰“ä¸‹å …å¯¦åŸºç¤ã€‚
