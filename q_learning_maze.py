import numpy as np
import random
import time
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import matplotlib.animation as animation
from IPython.display import clear_output
import os

from environment import Environment
from agent import Agent
from visualize import MazeVisualizer


def solve_maze_qlearning(maze, episodes=100, lr=0.7, gamma=0.9, epsilon=0.9, epsilon_decay=0.995):
    """ä½¿ç”¨Q-Learningè§£æ±ºè¿·å®®å•é¡Œ"""
    
    # æ‰¾åˆ°èµ·å§‹ä½ç½®
    start_pos = tuple(np.argwhere(maze == 2)[0])
    
    # å‰µå»ºç’°å¢ƒå’Œä»£ç†äºº
    environment = Environment(maze)
    agent = Agent(maze, start_pos)
    
    steps_history = []
    
    print(f"é–‹å§‹è¨“ç·´ - è¿·å®®å¤§å°: {maze.shape}")
    print(f"èµ·å§‹ä½ç½®: {start_pos}")
    
    for episode in range(episodes):
        agent.state = start_pos
        steps = 0
        current_epsilon = epsilon * (epsilon_decay ** episode)
        
        while True:
            steps += 1
            
            # é¸æ“‡å‹•ä½œ
            action = agent.getAction(current_epsilon)
            
            # åŸ·è¡Œå‹•ä½œ
            reward, nextState, result = environment.doAction(agent.state, action)
            
            # æ›´æ–°Qè¡¨æ ¼
            agent.updateQTable(action, nextState, reward, lr, gamma)
            
            # æ›´æ–°ç‹€æ…‹
            agent.state = nextState
            
            # æª¢æŸ¥æ˜¯å¦åˆ°é”çµ‚é»
            if result:
                steps_history.append(steps)
                if (episode + 1) % 10 == 0:
                    print(f'Episode {episode + 1:3d}: {steps:3d} steps to goal (Îµ={current_epsilon:.3f})')
                break
            
            # é˜²æ­¢ç„¡é™å¾ªç’°
            if steps > 1000:
                steps_history.append(1000)
                break
    
    return agent, steps_history

def get_optimal_path(agent, maze, show_steps=False):
    """ç²å–æœ€å„ªè·¯å¾‘"""
    start_pos = tuple(np.argwhere(maze == 2)[0])
    environment = Environment(maze)
    
    path = [start_pos]
    agent.state = start_pos
    
    if show_steps:
        print(f"\nğŸš¶ ä»£ç†äººç§»å‹•è·¯å¾‘:")
        print(f"Step 1: {start_pos} (èµ·é»)")
    
    for step in range(1000):  # é˜²æ­¢ç„¡é™å¾ªç’°
        action = agent.getAction(1.0)  # å®Œå…¨è²ªå©ªç­–ç•¥
        reward, nextState, result = environment.doAction(agent.state, action)
        agent.state = nextState
        path.append(nextState)
        
        if show_steps:
            status = "åˆ°é”çµ‚é»!" if result else f"ç§»å‹• {action}"
            print(f"Step {step + 2}: {nextState} ({status})")
        
        if result:
            break
    
    return path

def get_training_path(agent, maze, episode_num=None):
    """ç²å–è¨“ç·´éç¨‹ä¸­çš„ä¸€æ¬¡å®Œæ•´è·¯å¾‘ï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰"""
    start_pos = tuple(np.argwhere(maze == 2)[0])
    environment = Environment(maze)
    
    path = [start_pos]
    agent.state = start_pos
    
    print(f"\nğŸ® Episode {episode_num} è·¯å¾‘è¿½è¹¤:")
    print(f"èµ·é»: {start_pos}")
    
    for step in range(1000):  # é˜²æ­¢ç„¡é™å¾ªç’°
        action = agent.getAction(0.1)  # ä½æ¢ç´¢ç‡ä»¥å±•ç¤ºå­¸ç¿’æ•ˆæœ
        reward, nextState, result = environment.doAction(agent.state, action)
        agent.state = nextState
        path.append(nextState)
        
        print(f"Step {step + 1}: {action} -> {nextState} (reward: {reward})")
        
        if result:
            print(f"ğŸ¯ åˆ°é”çµ‚é»! ç¸½æ­¥æ•¸: {len(path)}")
            break
        
        if step > 50:  # é¿å…è¼¸å‡ºéé•·
            print("... (è·¯å¾‘éé•·ï¼Œçœç•¥ä¸­é–“æ­¥é©Ÿ)")
            break
    
    return path

def main():
    """ä¸»å‡½æ•¸"""
    
    # å®šç¾©å…©å€‹è¿·å®®
    maze_10x10 = np.array([
        [1,1,1,1,1,1,1,1,1,1,1,1],
        [1,1,1,1,0,0,0,0,0,2,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,0,1,1,1,1,0,1,1,1,1,1],
        [1,0,0,0,0,0,0,0,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,0,0,0,0,3,1,1,1,1],
        [1,1,1,1,1,1,1,1,1,1,1,1]
    ])

    maze_25x25 = np.array([
        [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],
        [1,1,2,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1],
        [1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,0,1,1,1],
        [1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,0,0,0,0,0,0,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],
        [1,1,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1],
        [1,1,0,0,0,0,0,1,1,1,0,1,1,0,1,1,1,1,0,0,0,0,0,0,0,1,1],
        [1,1,0,1,1,1,0,1,1,1,0,1,1,0,1,1,1,1,1,1,1,0,1,0,1,1,1],
        [1,1,0,1,1,1,0,1,0,0,0,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,0,0,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,1,0,0,0,0,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,0,0,0,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,0,1,0,1,1,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,1],
        [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
    ])
   
    # å‰µå»ºè¦–è¦ºåŒ–å·¥å…·
    viz_10x10 = MazeVisualizer(maze_10x10)
    viz_25x25 = MazeVisualizer(maze_25x25)
    
    print("=" * 60)
    print("Q-Learning è¿·å®®æ±‚è§£å™¨")
    print("=" * 60)
    
    # è§£æ±º10x10è¿·å®®
    print("\nğŸ¯ è§£æ±º 10x10 è¿·å®®...")
    agent_10x10, steps_10x10 = solve_maze_qlearning(
        maze_10x10, episodes=200, lr=0.7, gamma=0.9, epsilon=0.9
    )
    
    # ç²å–æœ€å„ªè·¯å¾‘ä¸¦é¡¯ç¤ºè©³ç´°æ­¥é©Ÿ
    print("\nğŸ“ ç²å–æœ€å„ªè·¯å¾‘...")
    optimal_path_10x10 = get_optimal_path(agent_10x10, maze_10x10, show_steps=True)
    
    print(f"\nâœ… 10x10è¿·å®®å®Œæˆ!")
    print(f"   æœ€çµ‚è·¯å¾‘é•·åº¦: {len(optimal_path_10x10)} æ­¥")
    print(f"   å¹³å‡æ­¥æ•¸(æœ€å¾Œ10æ¬¡): {np.mean(steps_10x10[-10:]):.1f}")
    
    # é¡¯ç¤ºæ§åˆ¶å°è·¯å¾‘è¦–è¦ºåŒ–
    viz_10x10.print_path_console(optimal_path_10x10, "10x10 è¿·å®®")
    
    # è§£æ±º25x25è¿·å®®
    print("\nğŸ¯ è§£æ±º 25x25 è¿·å®®...")
    agent_25x25, steps_25x25 = solve_maze_qlearning(
        maze_25x25, episodes=500, lr=0.5, gamma=0.95, epsilon=0.9
    )
    
    # ç²å–æœ€å„ªè·¯å¾‘
    print("\nğŸ“ ç²å–æœ€å„ªè·¯å¾‘...")
    optimal_path_25x25 = get_optimal_path(agent_25x25, maze_25x25, show_steps=False)  # 25x25å¤ªé•·ï¼Œä¸é¡¯ç¤ºè©³ç´°æ­¥é©Ÿ
    
    print(f"\nâœ… 25x25è¿·å®®å®Œæˆ!")
    print(f"   æœ€çµ‚è·¯å¾‘é•·åº¦: {len(optimal_path_25x25)} æ­¥")
    print(f"   å¹³å‡æ­¥æ•¸(æœ€å¾Œ10æ¬¡): {np.mean(steps_25x25[-10:]):.1f}")
    
    # é¡¯ç¤ºæ§åˆ¶å°è·¯å¾‘è¦–è¦ºåŒ–
    viz_25x25.print_path_console(optimal_path_25x25, "25x25 è¿·å®®")
    
    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“Š è¨“ç·´çµæœæ‘˜è¦")
    print("=" * 60)
    print(f"10x10 è¿·å®® - æœ€å„ªè§£: {len(optimal_path_10x10)} æ­¥")
    print(f"25x25 è¿·å®® - æœ€å„ªè§£: {len(optimal_path_25x25)} æ­¥")
    
    # è¦–è¦ºåŒ–çµæœï¼ˆå¯é¸ï¼‰
    try:
        print("\nğŸ“ˆ ç¹ªè£½å­¸ç¿’æ›²ç·š...")
        viz_10x10.plot_learning_curve(steps_10x10, "10x10 Maze Learning Progress")
        viz_25x25.plot_learning_curve(steps_25x25, "25x25 Maze Learning Progress")
        
        print("ğŸ—ºï¸  é¡¯ç¤ºè¿·å®®å’Œæœ€å„ªè·¯å¾‘...")
        viz_10x10.plot_maze("10x10 Maze with Optimal Path", optimal_path_10x10)
        viz_25x25.plot_maze("25x25 Maze with Optimal Path", optimal_path_25x25)
        
        # å‹•ç•«é¡¯ç¤º
        print("\nğŸ¬ å‰µå»ºå‹•ç•«...")
        user_input = input("æ˜¯å¦è¦é¡¯ç¤ºå‹•ç•«? (y/n): ").lower().strip()
        if user_input == 'y' or user_input == 'yes':
            print("æ­£åœ¨å‰µå»º 10x10 è¿·å®®å‹•ç•«...")
            anim_10x10 = viz_10x10.animate_path(optimal_path_10x10, "10x10 Maze Agent Animation")
            
            save_gif = input("æ˜¯å¦ä¿å­˜ç‚ºGIFæ–‡ä»¶? (y/n): ").lower().strip()
            if save_gif == 'y' or save_gif == 'yes':
                viz_10x10.animate_path(optimal_path_10x10, "10x10 Maze Agent Animation", 
                                     save_gif=True, filename="maze_10x10_animation.gif")
            
            # å°æ–¼25x25è¿·å®®ï¼Œç”±æ–¼è·¯å¾‘è¼ƒé•·ï¼Œè©¢å•æ˜¯å¦è¦é¡¯ç¤º
            show_25x25 = input("æ˜¯å¦è¦é¡¯ç¤º 25x25 è¿·å®®å‹•ç•«? (è·¯å¾‘è¼ƒé•·ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ™‚é–“) (y/n): ").lower().strip()
            if show_25x25 == 'y' or show_25x25 == 'yes':
                print("æ­£åœ¨å‰µå»º 25x25 è¿·å®®å‹•ç•«...")
                anim_25x25 = viz_25x25.animate_path(optimal_path_25x25, "25x25 Maze Agent Animation")
                
                save_gif_25 = input("æ˜¯å¦ä¿å­˜ 25x25 å‹•ç•«ç‚ºGIFæ–‡ä»¶? (y/n): ").lower().strip()
                if save_gif_25 == 'y' or save_gif_25 == 'yes':
                    viz_25x25.animate_path(optimal_path_25x25, "25x25 Maze Agent Animation", 
                                         save_gif=True, filename="maze_25x25_animation.gif")
        
    except Exception as e:
        print(f"è¦–è¦ºåŒ–åŠŸèƒ½éœ€è¦GUIç’°å¢ƒ: {e}")
    
    # æ¼”ç¤ºè¨“ç·´éç¨‹ä¸­çš„è·¯å¾‘è¿½è¹¤
    print("\nğŸ” æ¼”ç¤ºè¨“ç·´éç¨‹è·¯å¾‘è¿½è¹¤...")
    demo_input = input("æ˜¯å¦è¦çœ‹è¨“ç·´éç¨‹ä¸­çš„è·¯å¾‘æ¼”ç¤º? (y/n): ").lower().strip()
    if demo_input == 'y' or demo_input == 'yes':
        print("é‡æ–°å‰µå»ºä»£ç†äººé€²è¡Œæ¼”ç¤º...")
        demo_agent = Agent(maze_10x10, tuple(np.argwhere(maze_10x10 == 2)[0]))
        
        # ç°¡å–®è¨“ç·´å¹¾å€‹å›åˆ
        environment = Environment(maze_10x10)
        for episode in range(5):
            demo_agent.state = tuple(np.argwhere(maze_10x10 == 2)[0])
            for _ in range(100):
                action = demo_agent.getAction(0.8)
                reward, nextState, result = environment.doAction(demo_agent.state, action)
                demo_agent.updateQTable(action, nextState, reward)
                demo_agent.state = nextState
                if result:
                    break
        
        # é¡¯ç¤ºè¨“ç·´å¾Œçš„è·¯å¾‘
        demo_path = get_training_path(demo_agent, maze_10x10, "æ¼”ç¤º")
    
    return agent_10x10, agent_25x25, steps_10x10, steps_25x25

if __name__ == "__main__":
    main()
