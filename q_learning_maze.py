import numpy as np
import random
import time
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import matplotlib.animation as animation
from IPython.display import clear_output
import os

class Environment:
    """éŠæˆ²ç’°å¢ƒé¡åˆ¥ï¼Œè™•ç†è¿·å®®äº’å‹•é‚è¼¯"""
    
    def __init__(self, maze):
        self.maze = maze
    
    def getNextState(self, state, action):
        """æ ¹æ“šç•¶å‰ç‹€æ…‹å’Œå‹•ä½œè¨ˆç®—ä¸‹ä¸€å€‹ç‹€æ…‹"""
        row, column = state[0], state[1]
        
        if action == 'up':
            row -= 1
        elif action == 'down':
            row += 1
        elif action == 'left':
            column -= 1
        elif action == 'right':
            column += 1
            
        nextState = (row, column)
        
        try:
            # è¶…å‡ºé‚Šç•Œæˆ–æ’ç‰†
            if row < 0 or column < 0 or self.maze[row, column] == 1:
                return [state, False]
            # åˆ°é”çµ‚é»
            elif self.maze[row, column] == 3:
                return [nextState, True]
            # æ­£å¸¸ç§»å‹•
            else:
                return [nextState, False]
        except IndexError:
            # è¶…å‡ºé‚Šç•Œ
            return [state, False]
    
    def doAction(self, state, action):
        """åŸ·è¡Œå‹•ä½œä¸¦è¿”å›çå‹µã€ä¸‹ä¸€ç‹€æ…‹å’Œæ˜¯å¦çµæŸ"""
        nextState, result = self.getNextState(state, action)
        
        # æ²’æœ‰ç§»å‹•ï¼ˆæ’ç‰†æˆ–é‚Šç•Œï¼‰
        if nextState == state:
            reward = -10
        # åˆ°é”çµ‚é»
        elif result:
            reward = 100
        # æ­£å¸¸ç§»å‹•
        else:
            reward = -1
            
        return [reward, nextState, result]

class Agent:
    """Q-Learningä»£ç†äºº"""
    
    def __init__(self, maze, initState):
        self.maze = maze
        self.state = initState
        self.actionList = ['up', 'down', 'left', 'right']
        self.actionDict = {'up': 0, 'down': 1, 'left': 2, 'right': 3}
        self.initQTable()
    
    def initQTable(self):
        """åˆå§‹åŒ–Qè¡¨æ ¼"""
        Q = np.zeros(self.maze.shape).tolist()
        for i, row in enumerate(Q):
            for j, _ in enumerate(row):
                Q[i][j] = [0, 0, 0, 0]  # up, down, left, right
        self.QTable = np.array(Q, dtype='f')
    
    def getAction(self, eGreedy=0.8):
        """æ ¹æ“šÎµ-greedyç­–ç•¥é¸æ“‡å‹•ä½œ"""
        if random.random() > eGreedy:
            return random.choice(self.actionList)
        else:
            Qsa = self.QTable[self.state].tolist()
            return self.actionList[Qsa.index(max(Qsa))]
    
    def getNextMaxQ(self, nextState):
        """ç²å–ä¸‹ä¸€ç‹€æ…‹çš„æœ€å¤§Qå€¼"""
        return max(self.QTable[nextState])
    
    def updateQTable(self, action, nextState, reward, lr=0.7, gamma=0.9):
        """æ›´æ–°Qè¡¨æ ¼"""
        Qs = self.QTable[self.state]
        Qsa = Qs[self.actionDict[action]]
        Qs[self.actionDict[action]] = (1 - lr) * Qsa + lr * (reward + gamma * self.getNextMaxQ(nextState))

class MazeVisualizer:
    """è¿·å®®è¦–è¦ºåŒ–å·¥å…·"""
    
    def __init__(self, maze):
        self.maze = maze
        self.colors = ['white', 'black', 'green', 'red', 'lightblue', 'orange']  # path, wall, start, goal, visited, agent
        self.cmap = ListedColormap(self.colors)
    
    def plot_maze(self, title="Maze", path=None):
        """ç¹ªè£½è¿·å®®"""
        plt.figure(figsize=(10, 10))
        maze_display = self.maze.copy().astype(float)
        
        if path:
            for i, pos in enumerate(path):
                if i == 0:  # èµ·é»
                    continue
                elif i == len(path) - 1:  # çµ‚é»
                    continue
                elif maze_display[pos] == 0:
                    maze_display[pos] = 4  # è·¯å¾‘æ¨™è¨˜ (lightblue)
        
        plt.imshow(maze_display, cmap=self.cmap, vmin=0, vmax=5)
        plt.title(title)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ è·¯å¾‘æ­¥æ•¸æ¨™è¨˜
        if path:
            for i, (row, col) in enumerate(path):
                if i > 0 and i < len(path) - 1:  # ä¸æ¨™è¨˜èµ·é»å’Œçµ‚é»
                    plt.text(col, row, str(i), ha='center', va='center', 
                            fontsize=8, fontweight='bold', color='darkblue')
        
        plt.show()
    
    def plot_learning_curve(self, steps_history, title="Learning Progress"):
        """ç¹ªè£½å­¸ç¿’æ›²ç·š"""
        plt.figure(figsize=(12, 6))
        plt.plot(steps_history)
        plt.title(title)
        plt.xlabel('Episode')
        plt.ylabel('Steps to Goal')
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def animate_path(self, path, title="Agent Animation", save_gif=False, filename="maze_animation.gif"):
        """å‹•ç•«é¡¯ç¤ºä»£ç†äººç§»å‹•è·¯å¾‘"""
        fig, ax = plt.subplots(figsize=(10, 10))
        
        def animate(frame):
            ax.clear()
            maze_display = self.maze.copy().astype(float)
            
            # é¡¯ç¤ºå·²èµ°éçš„è·¯å¾‘
            for i in range(min(frame + 1, len(path))):
                pos = path[i]
                if i == 0:  # èµ·é»
                    continue
                elif i == len(path) - 1 and frame >= len(path) - 1:  # çµ‚é»
                    continue
                elif maze_display[pos] == 0:
                    maze_display[pos] = 4  # å·²è¨ªå•è·¯å¾‘
            
            # é¡¯ç¤ºç•¶å‰ä»£ç†äººä½ç½®
            if frame < len(path):
                current_pos = path[frame]
                if maze_display[current_pos] != 3:  # ä¸è¦†è“‹çµ‚é»
                    maze_display[current_pos] = 5  # ä»£ç†äººä½ç½® (orange)
            
            ax.imshow(maze_display, cmap=self.cmap, vmin=0, vmax=5)
            ax.set_title(f"{title} - Step {frame + 1}/{len(path)}")
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ æ­¥æ•¸æ¨™è¨˜
            if frame < len(path):
                current_pos = path[frame]
                ax.text(current_pos[1], current_pos[0], f"Step {frame + 1}", 
                       ha='center', va='bottom', fontsize=10, 
                       fontweight='bold', color='white',
                       bbox=dict(boxstyle="round,pad=0.3", facecolor='black', alpha=0.7))
        
        anim = animation.FuncAnimation(fig, animate, frames=len(path), 
                                     interval=500, repeat=True, blit=False)
        
        if save_gif:
            anim.save(filename, writer='pillow', fps=2)
            print(f"å‹•ç•«å·²ä¿å­˜ç‚º: {filename}")
        
        plt.show()
        return anim
    
    def print_path_console(self, path, maze_name="Maze"):
        """åœ¨æ§åˆ¶å°ä»¥ASCIIè—è¡“é¡¯ç¤ºè·¯å¾‘"""
        print(f"\nğŸ—ºï¸  {maze_name} è·¯å¾‘è¦–è¦ºåŒ–:")
        print("=" * 50)
        
        maze_display = self.maze.copy()
        
        # æ¨™è¨˜è·¯å¾‘
        for i, pos in enumerate(path):
            if i == 0:
                continue  # èµ·é»ä¿æŒåŸæ¨£
            elif i == len(path) - 1:
                continue  # çµ‚é»ä¿æŒåŸæ¨£
            elif maze_display[pos] == 0:
                maze_display[pos] = 9  # è·¯å¾‘æ¨™è¨˜
        
        # å­—ç¬¦æ˜ å°„
        char_map = {0: 'Â·', 1: 'â–ˆ', 2: 'S', 3: 'G', 9: 'â—‹'}
        
        for row in maze_display:
            line = ""
            for cell in row:
                line += char_map.get(cell, '?') + " "
            print(line)
        
        print("\nåœ–ä¾‹: S=èµ·é», G=çµ‚é», â—‹=è·¯å¾‘, â–ˆ=ç‰†å£, Â·=ç©ºåœ°")
        print(f"ç¸½æ­¥æ•¸: {len(path)} æ­¥")
        print("=" * 50)

def solve_maze_qlearning(maze, episodes=100, lr=0.7, gamma=0.9, epsilon=0.9, epsilon_decay=0.995):
    """ä½¿ç”¨Q-Learningè§£æ±ºè¿·å®®å•é¡Œ"""
    
    # æ‰¾åˆ°èµ·å§‹ä½ç½®
    start_pos = tuple(np.argwhere(maze == 2)[0])
    
    # å‰µå»ºç’°å¢ƒå’Œä»£ç†äºº
    environment = Environment(maze)
    agent = Agent(maze, start_pos)
    
    steps_history = []
    
    print(f"é–‹å§‹è¨“ç·´ - è¿·å®®å¤§å°: {maze.shape}")
    print(f"èµ·å§‹ä½ç½®: {start_pos}")
    
    for episode in range(episodes):
        agent.state = start_pos
        steps = 0
        current_epsilon = epsilon * (epsilon_decay ** episode)
        
        while True:
            steps += 1
            
            # é¸æ“‡å‹•ä½œ
            action = agent.getAction(current_epsilon)
            
            # åŸ·è¡Œå‹•ä½œ
            reward, nextState, result = environment.doAction(agent.state, action)
            
            # æ›´æ–°Qè¡¨æ ¼
            agent.updateQTable(action, nextState, reward, lr, gamma)
            
            # æ›´æ–°ç‹€æ…‹
            agent.state = nextState
            
            # æª¢æŸ¥æ˜¯å¦åˆ°é”çµ‚é»
            if result:
                steps_history.append(steps)
                if (episode + 1) % 10 == 0:
                    print(f'Episode {episode + 1:3d}: {steps:3d} steps to goal (Îµ={current_epsilon:.3f})')
                break
            
            # é˜²æ­¢ç„¡é™å¾ªç’°
            if steps > 1000:
                steps_history.append(1000)
                break
    
    return agent, steps_history

def get_optimal_path(agent, maze, show_steps=False):
    """ç²å–æœ€å„ªè·¯å¾‘"""
    start_pos = tuple(np.argwhere(maze == 2)[0])
    environment = Environment(maze)
    
    path = [start_pos]
    agent.state = start_pos
    
    if show_steps:
        print(f"\nğŸš¶ ä»£ç†äººç§»å‹•è·¯å¾‘:")
        print(f"Step 1: {start_pos} (èµ·é»)")
    
    for step in range(1000):  # é˜²æ­¢ç„¡é™å¾ªç’°
        action = agent.getAction(1.0)  # å®Œå…¨è²ªå©ªç­–ç•¥
        reward, nextState, result = environment.doAction(agent.state, action)
        agent.state = nextState
        path.append(nextState)
        
        if show_steps:
            status = "åˆ°é”çµ‚é»!" if result else f"ç§»å‹• {action}"
            print(f"Step {step + 2}: {nextState} ({status})")
        
        if result:
            break
    
    return path

def get_training_path(agent, maze, episode_num=None):
    """ç²å–è¨“ç·´éç¨‹ä¸­çš„ä¸€æ¬¡å®Œæ•´è·¯å¾‘ï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰"""
    start_pos = tuple(np.argwhere(maze == 2)[0])
    environment = Environment(maze)
    
    path = [start_pos]
    agent.state = start_pos
    
    print(f"\nğŸ® Episode {episode_num} è·¯å¾‘è¿½è¹¤:")
    print(f"èµ·é»: {start_pos}")
    
    for step in range(1000):  # é˜²æ­¢ç„¡é™å¾ªç’°
        action = agent.getAction(0.1)  # ä½æ¢ç´¢ç‡ä»¥å±•ç¤ºå­¸ç¿’æ•ˆæœ
        reward, nextState, result = environment.doAction(agent.state, action)
        agent.state = nextState
        path.append(nextState)
        
        print(f"Step {step + 1}: {action} -> {nextState} (reward: {reward})")
        
        if result:
            print(f"ğŸ¯ åˆ°é”çµ‚é»! ç¸½æ­¥æ•¸: {len(path)}")
            break
        
        if step > 50:  # é¿å…è¼¸å‡ºéé•·
            print("... (è·¯å¾‘éé•·ï¼Œçœç•¥ä¸­é–“æ­¥é©Ÿ)")
            break
    
    return path

def main():
    """ä¸»å‡½æ•¸"""
    
    # å®šç¾©å…©å€‹è¿·å®®
    maze_10x10 = np.array([
        [1,1,1,1,1,1,1,1,1,1,1,1],
        [1,1,1,1,0,0,0,0,0,2,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,0,1,1,1,1,0,1,1,1,1,1],
        [1,0,0,0,0,0,0,0,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,0,0,0,0,3,1,1,1,1],
        [1,1,1,1,1,1,1,1,1,1,1,1]
    ])

    maze_25x25 = np.array([
        [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],
        [1,1,2,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1],
        [1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,0,1,1,1],
        [1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,0,0,0,0,0,0,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],
        [1,1,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1],
        [1,1,1,1,1,1,0,1,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1],
        [1,1,0,0,0,0,0,1,1,1,0,1,1,0,1,1,1,1,0,0,0,0,0,0,0,1,1],
        [1,1,0,1,1,1,0,1,1,1,0,1,1,0,1,1,1,1,1,1,1,0,1,0,1,1,1],
        [1,1,0,1,1,1,0,1,0,0,0,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,0,0,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,1,0,0,0,0,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,0,0,0,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,0,1,1,1,1,1,0,1,0,1,1,1,1,0,1,1,1,0,1,1,1,1,1],
        [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,1],
        [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
    ])
    
    # å‰µå»ºè¦–è¦ºåŒ–å·¥å…·
    viz_10x10 = MazeVisualizer(maze_10x10)
    viz_25x25 = MazeVisualizer(maze_25x25)
    
    print("=" * 60)
    print("Q-Learning è¿·å®®æ±‚è§£å™¨")
    print("=" * 60)
    
    # è§£æ±º10x10è¿·å®®
    print("\nğŸ¯ è§£æ±º 10x10 è¿·å®®...")
    agent_10x10, steps_10x10 = solve_maze_qlearning(
        maze_10x10, episodes=200, lr=0.7, gamma=0.9, epsilon=0.9
    )
    
    # ç²å–æœ€å„ªè·¯å¾‘ä¸¦é¡¯ç¤ºè©³ç´°æ­¥é©Ÿ
    print("\nğŸ“ ç²å–æœ€å„ªè·¯å¾‘...")
    optimal_path_10x10 = get_optimal_path(agent_10x10, maze_10x10, show_steps=True)
    
    print(f"\nâœ… 10x10è¿·å®®å®Œæˆ!")
    print(f"   æœ€çµ‚è·¯å¾‘é•·åº¦: {len(optimal_path_10x10)} æ­¥")
    print(f"   å¹³å‡æ­¥æ•¸(æœ€å¾Œ10æ¬¡): {np.mean(steps_10x10[-10:]):.1f}")
    
    # é¡¯ç¤ºæ§åˆ¶å°è·¯å¾‘è¦–è¦ºåŒ–
    viz_10x10.print_path_console(optimal_path_10x10, "10x10 è¿·å®®")
    
    # è§£æ±º25x25è¿·å®®
    print("\nğŸ¯ è§£æ±º 25x25 è¿·å®®...")
    agent_25x25, steps_25x25 = solve_maze_qlearning(
        maze_25x25, episodes=500, lr=0.5, gamma=0.95, epsilon=0.9
    )
    
    # ç²å–æœ€å„ªè·¯å¾‘
    print("\nğŸ“ ç²å–æœ€å„ªè·¯å¾‘...")
    optimal_path_25x25 = get_optimal_path(agent_25x25, maze_25x25, show_steps=False)  # 25x25å¤ªé•·ï¼Œä¸é¡¯ç¤ºè©³ç´°æ­¥é©Ÿ
    
    print(f"\nâœ… 25x25è¿·å®®å®Œæˆ!")
    print(f"   æœ€çµ‚è·¯å¾‘é•·åº¦: {len(optimal_path_25x25)} æ­¥")
    print(f"   å¹³å‡æ­¥æ•¸(æœ€å¾Œ10æ¬¡): {np.mean(steps_25x25[-10:]):.1f}")
    
    # é¡¯ç¤ºæ§åˆ¶å°è·¯å¾‘è¦–è¦ºåŒ–
    viz_25x25.print_path_console(optimal_path_25x25, "25x25 è¿·å®®")
    
    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“Š è¨“ç·´çµæœæ‘˜è¦")
    print("=" * 60)
    print(f"10x10 è¿·å®® - æœ€å„ªè§£: {len(optimal_path_10x10)} æ­¥")
    print(f"25x25 è¿·å®® - æœ€å„ªè§£: {len(optimal_path_25x25)} æ­¥")
    
    # è¦–è¦ºåŒ–çµæœï¼ˆå¯é¸ï¼‰
    try:
        print("\nğŸ“ˆ ç¹ªè£½å­¸ç¿’æ›²ç·š...")
        viz_10x10.plot_learning_curve(steps_10x10, "10x10 Maze Learning Progress")
        viz_25x25.plot_learning_curve(steps_25x25, "25x25 Maze Learning Progress")
        
        print("ğŸ—ºï¸  é¡¯ç¤ºè¿·å®®å’Œæœ€å„ªè·¯å¾‘...")
        viz_10x10.plot_maze("10x10 Maze with Optimal Path", optimal_path_10x10)
        viz_25x25.plot_maze("25x25 Maze with Optimal Path", optimal_path_25x25)
        
        # å‹•ç•«é¡¯ç¤º
        print("\nğŸ¬ å‰µå»ºå‹•ç•«...")
        user_input = input("æ˜¯å¦è¦é¡¯ç¤ºå‹•ç•«? (y/n): ").lower().strip()
        if user_input == 'y' or user_input == 'yes':
            print("æ­£åœ¨å‰µå»º 10x10 è¿·å®®å‹•ç•«...")
            anim_10x10 = viz_10x10.animate_path(optimal_path_10x10, "10x10 Maze Agent Animation")
            
            save_gif = input("æ˜¯å¦ä¿å­˜ç‚ºGIFæ–‡ä»¶? (y/n): ").lower().strip()
            if save_gif == 'y' or save_gif == 'yes':
                viz_10x10.animate_path(optimal_path_10x10, "10x10 Maze Agent Animation", 
                                     save_gif=True, filename="maze_10x10_animation.gif")
            
            # å°æ–¼25x25è¿·å®®ï¼Œç”±æ–¼è·¯å¾‘è¼ƒé•·ï¼Œè©¢å•æ˜¯å¦è¦é¡¯ç¤º
            show_25x25 = input("æ˜¯å¦è¦é¡¯ç¤º 25x25 è¿·å®®å‹•ç•«? (è·¯å¾‘è¼ƒé•·ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ™‚é–“) (y/n): ").lower().strip()
            if show_25x25 == 'y' or show_25x25 == 'yes':
                print("æ­£åœ¨å‰µå»º 25x25 è¿·å®®å‹•ç•«...")
                anim_25x25 = viz_25x25.animate_path(optimal_path_25x25, "25x25 Maze Agent Animation")
                
                save_gif_25 = input("æ˜¯å¦ä¿å­˜ 25x25 å‹•ç•«ç‚ºGIFæ–‡ä»¶? (y/n): ").lower().strip()
                if save_gif_25 == 'y' or save_gif_25 == 'yes':
                    viz_25x25.animate_path(optimal_path_25x25, "25x25 Maze Agent Animation", 
                                         save_gif=True, filename="maze_25x25_animation.gif")
        
    except Exception as e:
        print(f"è¦–è¦ºåŒ–åŠŸèƒ½éœ€è¦GUIç’°å¢ƒ: {e}")
    
    # æ¼”ç¤ºè¨“ç·´éç¨‹ä¸­çš„è·¯å¾‘è¿½è¹¤
    print("\nğŸ” æ¼”ç¤ºè¨“ç·´éç¨‹è·¯å¾‘è¿½è¹¤...")
    demo_input = input("æ˜¯å¦è¦çœ‹è¨“ç·´éç¨‹ä¸­çš„è·¯å¾‘æ¼”ç¤º? (y/n): ").lower().strip()
    if demo_input == 'y' or demo_input == 'yes':
        print("é‡æ–°å‰µå»ºä»£ç†äººé€²è¡Œæ¼”ç¤º...")
        demo_agent = Agent(maze_10x10, tuple(np.argwhere(maze_10x10 == 2)[0]))
        
        # ç°¡å–®è¨“ç·´å¹¾å€‹å›åˆ
        environment = Environment(maze_10x10)
        for episode in range(5):
            demo_agent.state = tuple(np.argwhere(maze_10x10 == 2)[0])
            for _ in range(100):
                action = demo_agent.getAction(0.8)
                reward, nextState, result = environment.doAction(demo_agent.state, action)
                demo_agent.updateQTable(action, nextState, reward)
                demo_agent.state = nextState
                if result:
                    break
        
        # é¡¯ç¤ºè¨“ç·´å¾Œçš„è·¯å¾‘
        demo_path = get_training_path(demo_agent, maze_10x10, "æ¼”ç¤º")
    
    return agent_10x10, agent_25x25, steps_10x10, steps_25x25

if __name__ == "__main__":
    main()
